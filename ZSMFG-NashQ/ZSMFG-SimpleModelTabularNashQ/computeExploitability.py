import numpy as np
from tqdm import tqdm
from myEnv import my1dGridEnv
from myTable import myQTable
from NashQLearner import NashQPlayer

def compute_exploitability(steps,Players,Q_1,Q_2,pi_1,pi_2,env):

    r_t_1=Players.value_iterations(Q_1,Q_2,pi_2,env,steps,Q_1_fixed=False)
    r_t_2=Players.value_iterations(Q_2,Q_1,pi_1,env,steps,Q_1_fixed=True)


    
    return  r_t_1,r_t_2

env = my1dGridEnv()
file = np.load("ZSMFG-NashQ/historyTables/corrected/Q_MC_zeros_ecos_2action_results_iter5000.npz")
Q_1 = file["Q_1"]
Q_2 = file["Q_2"]
            
if __name__=="__main__":
    
    Players = NashQPlayer(env,Q_1_table=myQTable(history_table=Q_1),Q_2_table = myQTable(history_table=Q_2))
    evolve_states,pi_1,pi_2,reward_1,reward_2 = Players.recover_equilibrium_policy(100,Q_1=Players.Q_1,Q_2=Players.Q_2,env=env,simple_recover=True)
    #print(evolve_states)
    _,whole_pi_1,whole_pi_2,_,_=Players.recover_equilibrium_policy(100,Q_1=Players.Q_1,Q_2=Players.Q_2,env=env,simple_recover=False)
    explo_value_1,explo_value_2 = compute_exploitability(100,Players,Players.Q_1,Players.Q_2,whole_pi_1,whole_pi_2,env)
    np.savez("ZSMFG-NashQ/ZSMFG-SimpleModelTabularNashQ/policies&exploitability/evaluation_100steps",evolution=evolve_states,reward_1_episode=reward_1,reward_2_episode=reward_2
    ,whole_pi_1=whole_pi_1,whole_pi_2=whole_pi_2)
    #explo_value_1,explo_value_2 = compute_exploitability(100,Players,Players.Q_1,Players.Q_2,pi_1,pi_2,env)
    #print(reward_1,reward_2)
    #print(explo_value_1,explo_value_2)
    # np.savez("ZSMFG-NashQ/ZSMFG-SimpleModelTabularNashQ/policies&exploitability/random_pi_explo", 
    # pi_1=pi_1,pi_2=pi_2,explo_1=explo_value_1,explo_2=explo_value_2,steps=100)
